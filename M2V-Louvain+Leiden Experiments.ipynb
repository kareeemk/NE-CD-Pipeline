{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Analysis of Static Community Detection Using M2V\n",
    "\n",
    "Running both the Louvain and Leiden algorithms multiple times and recording various statistics for each run can provide valuable insights for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Edge List w/ Weights to NetworkX\n",
    "\n",
    "NetworkX's read_weighted_edgelist function expects a simple text file with lines of the form <node1> <node2> <weight>, without headers. Since our data is in CSV format, you'll need to use Pandas (or another method) to load the CSV and adjust it to become readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       source  target    weight\n",
      "0          p0      a1  0.752102\n",
      "1          p0      a2  0.803198\n",
      "2          p0      a3  0.900095\n",
      "3          p0      a4  0.682413\n",
      "4          p0      a5  0.833705\n",
      "...       ...     ...       ...\n",
      "51372  p10169   p8094  0.209173\n",
      "51373  p10169   p7974  0.235403\n",
      "51374  p10169   p5852  0.265217\n",
      "51375  p10169  p10113  0.639329\n",
      "51376  p10169  p10031  0.371488\n",
      "\n",
      "[51377 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "edge_list_df = pd.read_csv('New Input/M2V_edge_list_with_similarity.csv')\n",
    "\n",
    "print(edge_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges with negative weights: 1981\n"
     ]
    }
   ],
   "source": [
    "negative_weights = edge_list_df[edge_list_df['weight'] < 0]\n",
    "print(f\"Number of edges with negative weights: {len(negative_weights)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Louvain is not made to consider negative edge weights, we will rescale the weights such that instead of [-1, 1] being the range, it is now [0, 1], where 0 now represents perfect dissimilarity, 0.5 represents orthogonality, and 1 represents perfect similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       source  target    weight\n",
      "0          p0      a1  0.876051\n",
      "1          p0      a2  0.901599\n",
      "2          p0      a3  0.950047\n",
      "3          p0      a4  0.841207\n",
      "4          p0      a5  0.916853\n",
      "...       ...     ...       ...\n",
      "51372  p10169   p8094  0.604586\n",
      "51373  p10169   p7974  0.617701\n",
      "51374  p10169   p5852  0.632609\n",
      "51375  p10169  p10113  0.819664\n",
      "51376  p10169  p10031  0.685744\n",
      "\n",
      "[51377 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "edge_list_df['weight'] = (edge_list_df['weight'] + 1) / 2\n",
    "\n",
    "print(edge_list_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue with the creation of a graph, NetworkX specifies that an undirected, weighted graph must not have self-loop, parallel edges (A->B, B->A), or duplicate edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate edges: 0\n",
      "Number of self-loops: 4\n",
      "source    0\n",
      "target    0\n",
      "weight    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "duplicate_edges = edge_list_df.duplicated(subset=['source', 'target'], keep=False)\n",
    "print(f\"Number of duplicate edges: {duplicate_edges.sum()}\")\n",
    "\n",
    "self_loops = edge_list_df[edge_list_df['source'] == edge_list_df['target']]\n",
    "print(f\"Number of self-loops: {len(self_loops)}\")\n",
    "\n",
    "print(edge_list_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges before dropping self-loops: 51377\n",
      "Number of edges after dropping self-loops: 51373\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of edges before dropping self-loops: {len(edge_list_df)}\")\n",
    "\n",
    "edge_list_df = edge_list_df[edge_list_df['source'] != edge_list_df['target']]\n",
    "\n",
    "print(f\"Number of edges after dropping self-loops: {len(edge_list_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [source, target, weight]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Find duplicate edges (ignoring the weight column)\n",
    "duplicate_edges = edge_list_df.duplicated(subset=['source', 'target'], keep=False)\n",
    "\n",
    "# Filter to get only the duplicate edges\n",
    "parallel_edges_df = edge_list_df[duplicate_edges]\n",
    "\n",
    "# Sort to better visualize parallel edges\n",
    "parallel_edges_sorted = parallel_edges_df.sort_values(by=['source', 'target'])\n",
    "\n",
    "print(parallel_edges_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Undirected Weighted Graph\n",
    "\n",
    "We iterate over the edge list DataFrame rows to add edges along with their weights to a new NetworkX graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def get_graph_info(graph):\n",
    "    print(\"Number of nodes:\", graph.number_of_nodes())\n",
    "    print(\"Number of edges:\", graph.number_of_edges())\n",
    "    \n",
    "    # Checking the graph type to provide appropriate information\n",
    "    if isinstance(graph, nx.DiGraph):\n",
    "        print(\"Graph is Directed\")\n",
    "    else:\n",
    "        print(\"Graph is Undirected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new graph\n",
    "G = nx.MultiGraph()\n",
    "\n",
    "# Add edges and weights\n",
    "for index, row in edge_list_df.iterrows():\n",
    "    source = row['source']\n",
    "    target = row['target']\n",
    "    weight = row['weight']\n",
    "    \n",
    "    # Add the edge with weight\n",
    "    G.add_edge(source, target, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 15649\n",
      "Number of edges: 51373\n",
      "Graph is Undirected\n"
     ]
    }
   ],
   "source": [
    "get_graph_info(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Modularity, Run Time and No. of Communities\n",
    "\n",
    "- Iterates 10 times, running both the Louvain and Leiden using CDLib on each iteration.\n",
    "- Records modularity, number of communities, and execution time for each run.\n",
    "- Calculates the average modularity, average number of communities, and average execution time for both algorithms across all runs.\n",
    "- Stores all this information in the results dictionary for easy access and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'bayanpy', 'wurlitzer', 'graph_tool', 'infomap'}\n",
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'pyclustering', 'ASLPAw'}\n",
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'wurlitzer', 'infomap'}\n"
     ]
    }
   ],
   "source": [
    "from cdlib import algorithms\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the data structure for results\n",
    "results = {\n",
    "    \"Louvain\": {\"modularity\": [], \"communities\": [], \"time\": []},\n",
    "    \"Leiden\": {\"modularity\": [], \"communities\": [], \"time\": []}\n",
    "}\n",
    "\n",
    "# Execute each algorithm 10 times\n",
    "for _ in range(10):\n",
    "    # Louvain\n",
    "    start_time = time.time()\n",
    "    communities_louvain = algorithms.louvain(G, weight='weight')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    results[\"Louvain\"][\"modularity\"].append(communities_louvain.newman_girvan_modularity().score)\n",
    "    results[\"Louvain\"][\"communities\"].append(len(communities_louvain.communities))\n",
    "    results[\"Louvain\"][\"time\"].append(elapsed_time)\n",
    "    \n",
    "    # Leiden\n",
    "    start_time = time.time()\n",
    "    communities_leiden = algorithms.leiden(G, weights='weight')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    results[\"Leiden\"][\"modularity\"].append(communities_leiden.newman_girvan_modularity().score)\n",
    "    results[\"Leiden\"][\"communities\"].append(len(communities_leiden.communities))\n",
    "    results[\"Leiden\"][\"time\"].append(elapsed_time)\n",
    "\n",
    "# Calculate averages\n",
    "for method in results:\n",
    "    results[method][\"avg_modularity\"] = np.mean(results[method][\"modularity\"])\n",
    "    results[method][\"avg_time\"] = np.mean(results[method][\"time\"])\n",
    "    results[method][\"avg_communities\"] = np.mean(results[method][\"communities\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Louvain': {'modularity': [0.5800333293129618, 0.6045040024210548, 0.6029962247636718, 0.6066544990866413, 0.5675916203008841, 0.5804464258097656, 0.6009444677169751, 0.6113157722888504, 0.637735280274994, 0.5612300136034208], 'communities': [1841, 1709, 1726, 1751, 1970, 1832, 1745, 1654, 1480, 1907], 'time': [8.318804025650024, 7.781469821929932, 6.852544069290161, 6.3088109493255615, 6.539769887924194, 8.105946063995361, 5.631251811981201, 7.129033088684082, 6.118636131286621, 6.201063871383667], 'avg_modularity': 0.5953451635579219, 'avg_time': 6.898732972145081, 'avg_communities': 1761.5}, 'Leiden': {'modularity': [0.7193660643959252, 0.7170395808763892, 0.7228124598741187, 0.725754038114919, 0.7274157423487653, 0.7142585122586171, 0.7237329133587553, 0.7202067617237046, 0.7232845198078207, 0.7177469060694326], 'communities': [26, 27, 30, 29, 31, 28, 30, 29, 28, 31], 'time': [1.4310641288757324, 1.300278902053833, 1.4258060455322266, 1.2655999660491943, 1.4199981689453125, 1.426102876663208, 1.4408159255981445, 1.275712013244629, 1.4496769905090332, 1.3140599727630615], 'avg_modularity': 0.7211617498828449, 'avg_time': 1.3749114990234375, 'avg_communities': 28.9}}\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Algorithm  Run  Modularity  Communities  Time (s)\n",
      "0    Louvain    1    0.580033         1841  8.318804\n",
      "1    Louvain    2    0.604504         1709  7.781470\n",
      "2    Louvain    3    0.602996         1726  6.852544\n",
      "3    Louvain    4    0.606654         1751  6.308811\n",
      "4    Louvain    5    0.567592         1970  6.539770\n",
      "5    Louvain    6    0.580446         1832  8.105946\n",
      "6    Louvain    7    0.600944         1745  5.631252\n",
      "7    Louvain    8    0.611316         1654  7.129033\n",
      "8    Louvain    9    0.637735         1480  6.118636\n",
      "9    Louvain   10    0.561230         1907  6.201064\n",
      "10    Leiden    1    0.719366           26  1.431064\n",
      "11    Leiden    2    0.717040           27  1.300279\n",
      "12    Leiden    3    0.722812           30  1.425806\n",
      "13    Leiden    4    0.725754           29  1.265600\n",
      "14    Leiden    5    0.727416           31  1.419998\n",
      "15    Leiden    6    0.714259           28  1.426103\n",
      "16    Leiden    7    0.723733           30  1.440816\n",
      "17    Leiden    8    0.720207           29  1.275712\n",
      "18    Leiden    9    0.723285           28  1.449677\n",
      "19    Leiden   10    0.717747           31  1.314060\n",
      "  Algorithm  Avg. Modularity  Avg. Communities  Avg. Time (s)\n",
      "0   Louvain         0.595345            1761.5       6.898733\n",
      "1    Leiden         0.721162              28.9       1.374911\n"
     ]
    }
   ],
   "source": [
    "# Convert the results dictionary into a pandas DataFrame\n",
    "# First, prepare the data in a structured form\n",
    "data = {\n",
    "    \"Algorithm\": [],\n",
    "    \"Run\": [],\n",
    "    \"Modularity\": [],\n",
    "    \"Communities\": [],\n",
    "    \"Time (s)\": []\n",
    "}\n",
    "\n",
    "# Populate the structured data from the results\n",
    "for algo in results:\n",
    "    for run in range(10):  # Assuming 10 runs as previously set\n",
    "        data[\"Algorithm\"].append(algo)\n",
    "        data[\"Run\"].append(run + 1)  # Run number (1-10)\n",
    "        data[\"Modularity\"].append(results[algo][\"modularity\"][run])\n",
    "        data[\"Communities\"].append(results[algo][\"communities\"][run])\n",
    "        data[\"Time (s)\"].append(results[algo][\"time\"][run])\n",
    "\n",
    "# Creating the DataFrame\n",
    "results_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame for visual inspection\n",
    "print(results_df)\n",
    "\n",
    "# Additionally, creating a summary DataFrame for averages\n",
    "summary_data = {\n",
    "    \"Algorithm\": [\"Louvain\", \"Leiden\"],\n",
    "    \"Avg. Modularity\": [results[\"Louvain\"][\"avg_modularity\"], results[\"Leiden\"][\"avg_modularity\"]],\n",
    "    \"Avg. Communities\": [results[\"Louvain\"][\"avg_communities\"], results[\"Leiden\"][\"avg_communities\"]],\n",
    "    \"Avg. Time (s)\": [results[\"Louvain\"][\"avg_time\"], results[\"Leiden\"][\"avg_time\"]]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Display the summary DataFrame\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = 'M2V_CD_Experiment_Values.csv'\n",
    "results_df.to_csv(csv_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
